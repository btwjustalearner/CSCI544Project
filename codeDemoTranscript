overview:
In this project we are trying to stack the insights given by finance news and time series forecasts.
Let's see what interesting result we would get from this NLP+traditional statistics model.

data collection:

For the news data part(这是为什么我们根据新闻数据来搜集股票数据):
It is harder to target a financial news dataset of Chinese than that of English, also Collectings 
stock price data is easier than finding financial news data. We put a focus on finding the proper
Chniese financial news dataset.  


The Chinese corpus we accessed is news on public companies in China in 2019 which has17149 records with 
date, code, company, title and text. The English corpus has 1 million records on 4 companies (Amazon, 
Apple, Google and Facebook) with publish date, title and text. English corpus is much larger than Chinese.
We took a sample from English corpus to deal with this problem. There are more companies in Chinese corpuus
but it shouldn't have too big impact since each record is a (company, date) pair rather than company itself.

Stock data:



data cleaning:


We have filtered the news data since we want to have more information for each date point to do 
a better prediction. We take a date into acount when it has more than 2 news for the same
company on that day. For company, the company has more than 1 valid date point will be considered 
as valid company.  

(run the scripts of extraction)

This process produce 2 files. The ChineseCompany_2.csv provides information for collecting Stock 
price. It tells the company and related time series of which the stock price should be collected.
Another file /data/traindata(ch).csv is the filtered training data.

We changed the column names to English and convert the Chinese date format to date. We also removed irrelevant 
columns.


time series model:
Now we have the news data and ground truth. How do we gather them together?

We tried several time series models and since our main model is NLP we balanced efficiency and accuracy.
ARIMA is a widely used and fast model. To gather ground truth for each record in traindata, we pick the  
close price for the news date or the closest next date if the news date is a weekend. And the last close
price. We pick the lastest 30 days and applied the model. It takes about 1 hour to inpute the whole traindata
so we won't run it now. Here is the result.

We are experimenting different ways to combine the two models, including linear combination, multilabel
classification, etc. And we are going to talk about them later

Let's talk about the main NLP part.

Final transcript for Yan:
(proposal)
In this project we are trying to stack the insights given by finance news and time series forecasts.
Let's see what interesting result we would get from this NLP+traditional statistics model.

(chinese raw train data)
We came up with this topic ourselves without any data in hand and we underestiimated the difficulty of 
collecting data and processing them. Our datasets are massive with Chinese train and test finance news data,
English news data and stock data. 

We took a sample from English corpus to deal with the dataset size difference problem. And since 
each record is a (company, date) pair rather than company itself, company number shouldn't matter too much.

We are going to focus on finding the proper Chinese finance news dataset today. 
We'll demonstrate stock data collecting later.

(cleaning)
We first convert all the column names and date format to English.
We keep a (company, date) pair only if the frequency is higher than 2. 

(train ch)
Now we have the news data and ground truth. How do we gather them together?

(time series)
We tried several time series models and since our main model is NLP we balanced efficiency and accuracy.

(time series production)
ARIMA is a widely used and fast model. To gather ground truth for each record in traindata, we pick the  
close price for the news date or the closest next date if the news date is a weekend. And the last close
price. We pick the lastest 30 days and applied the model. It takes about 1 hour to inpute the whole traindata
so we won't run it now. Here is the result.

We are experimenting different ways to combine the two models, including linear combination, multilabel
classification, etc. And we are going to talk about them later

Let's talk about the main NLP part.




